{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixIhiRd8BJEC"
      },
      "source": [
        "\n",
        "# **Assignment: Build Your First Image Classifier with a CNN**\n",
        "\n",
        "Welcome to your first hands-on assignment! In this notebook, you will build a complete Convolutional Neural Network (CNN) from scratch to classify images of cats and dogs.\n",
        "\n",
        "**Your goal is to complete the sections marked with `TODO`.** You will be responsible for defining the data generators, building the core CNN architecture, compiling it, and training it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwLbBqgpgKh2"
      },
      "source": [
        "### **0. Getting Started: Environment Setup & Requirements**\n",
        "\n",
        "Before you begin, you need to set up your environment. We highly recommend using Google Colab for this assignment as it provides a free GPU, which will make training your model much faster.\n",
        "\n",
        "#### **Instructions for Google Colab (Recommended):**\n",
        "\n",
        "1.  **Open in Colab:** Make sure you have this notebook open in Google Colab.\n",
        "2.  **Enable GPU:**\n",
        "    *   Go to the menu: `Runtime` -> `Change runtime type`.\n",
        "    *   In the \"Hardware accelerator\" dropdown, select **`GPU`** and click `Save`. This is crucial for deep learning!\n",
        "\n",
        "#### **Kaggle API Setup (Required for Everyone):**\n",
        "\n",
        "This notebook downloads its dataset directly from Kaggle. To do this, you need a Kaggle API token.\n",
        "\n",
        "1.  **Get your Token:**\n",
        "    *   Go to your Kaggle account page: [https://www.kaggle.com/me/account](https://www.kaggle.com/me/account)\n",
        "    *   Scroll down to the \"API\" section.\n",
        "    *   Click on **`Create New API Token`**. This will download a file named `kaggle.json`.\n",
        "\n",
        "2.  **Upload to Colab:**\n",
        "    *   On the left-hand side of your Colab window, click the **folder icon** to open the file browser.\n",
        "    *   Click the **`Upload to session storage`** icon (it looks like a page with an up arrow).\n",
        "    *   Select the `kaggle.json` file you just downloaded.\n",
        "\n",
        "You are now ready to begin! The code in the next few cells will automatically move this `kaggle.json` file to the correct location for the Kaggle API to find it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l3eCqdP8Vyw"
      },
      "source": [
        "# **1. Data Preparation**\n",
        "\n",
        "\n",
        "In this first major step, we will download our data, unzip it, and organize it into a clean directory structure. A well-organized dataset is the foundation of any successful machine learning project.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wSKETFR_9xI"
      },
      "source": [
        "#### **1.1 Imports**\n",
        "We'll start by importing all the essential libraries we will need.\n",
        "\n",
        "\n",
        "**TensorFlow**: Imports the TensorFlow library for machine learning.\n",
        "\n",
        "**Keras**: Imports the high-level Keras API from TensorFlow.\n",
        "\n",
        "**NumPy:** Imports the NumPy library for numerical computations.\n",
        "\n",
        "**Matplotlib:** Imports the Matplotlib library for data visualization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D3Kif9kaPQx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import ModelCheckpoint as MCP, EarlyStopping as ES\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TDqyUVlCJDZ"
      },
      "source": [
        "#### **1.2. Load and Extract the Data**\n",
        "Now, let's download the dataset from Kaggle and unzip it. The following code cells will handle this for you that :\n",
        "\n",
        "Loads dataset using Kaggle API\n",
        "```\n",
        "!kaggle datasets download -d shaunthesheep/microsoft-catsvsdogs-dataset\n",
        "```\n",
        "\n",
        "Also creates a dir 'kaggle' inside '/content/' to store\n",
        "* kaggle.json\n",
        "* Testing & Training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS5mbR1nZMfD"
      },
      "outputs": [],
      "source": [
        "#making a dir kaggle in the content folder\n",
        "os.mkdir(\"/content/kaggle/\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSF1RkASbCDd"
      },
      "outputs": [],
      "source": [
        "#shifting data of kaggle.json into newly created  dir\n",
        "copyfile(\"/content/kaggle.json\",\"/content/kaggle/kaggle.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5nvDK8lzX9J"
      },
      "outputs": [],
      "source": [
        "\n",
        "!kaggle datasets download -d shaunthesheep/microsoft-catsvsdogs-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXF0fDoe5otR"
      },
      "source": [
        "\n",
        "#### **1.3. Creating a Structured Workspace**\n",
        "Machine learning models, especially in Keras, work best when your data is organized. We need a main directory with `Train` and `Test` sub-folders, and inside each of those, a separate folder for each class (`Cats` and `Dogs`). The code below builds this structure for us.\n",
        "\n",
        "But note that After downloading the dataset from Kaggle, it arrives as a compressed ZIP file. To use the dataset in our project, we need to extract the files from this ZIP archive. The following steps illustrate how this is done using Python's `zipfile` module:\n",
        "\n",
        "1. **Importing the `zipfile` Module**:\n",
        "   - The `zipfile` module is imported and given the alias `z`. This module provides tools for reading and writing ZIP files.\n",
        "\n",
        "2. **Opening the ZIP File**:\n",
        "   - A `ZipFile` object is created to access the ZIP file located at `'/content/microsoft-catsvsdogs-dataset.zip'`. This object represents the ZIP archive and allows us to interact with its contents.\n",
        "\n",
        "3. **Extracting the Files**:\n",
        "   - The `extractall()` method is used to unpack all the files from the ZIP archive into the current working directory. This step makes the contents of the ZIP file accessible for further processing.\n",
        "\n",
        "4. **Closing the ZIP File**:\n",
        "   - After extraction, the `close()` method is called to close the `ZipFile` object. This is an important step to release system resources and avoid potential issues with file handling.\n",
        "\n",
        "By executing these steps, the dataset is extracted and ready for use in training and evaluating our CNN model for classifying images of cats and dogs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh_CTcW92Olk"
      },
      "outputs": [],
      "source": [
        "import zipfile as z\n",
        "zp=z.ZipFile('/content/microsoft-catsvsdogs-dataset.zip')\n",
        "zp.extractall()\n",
        "zp.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm1TmnZx51tk"
      },
      "source": [
        "\n",
        "To organize the dataset effectively for training and testing our Convolutional Neural Network (CNN), we need to create a structured directory hierarchy. This hierarchy will help us manage and access the images for both training and testing purposes.\n",
        "\n",
        "Here's the Explanation of the Code\n",
        "\n",
        "1. **Creating the Main Directory**:\n",
        "   - `os.mkdir(\"/content/kaggle/Cats-vs-Dogs\")`: This command creates a top-level directory named `Cats-vs-Dogs` within the `/content/kaggle/` path. This directory will serve as the root folder for our dataset organization.\n",
        "\n",
        "2. **Creating Subdirectories for Training and Testing**:\n",
        "   - `os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Train/\")`: This creates a `Train` folder inside the `Cats-vs-Dogs` directory, which will hold the training images.\n",
        "   - `os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Test/\")`: Similarly, this creates a `Test` folder inside the `Cats-vs-Dogs` directory for storing testing images.\n",
        "\n",
        "3. **Creating Category-Specific Folders in Training Data**:\n",
        "   - `os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Train/Cats\")`: This creates a `Cats` folder inside the `Train` directory to store images of cats used for training.\n",
        "   - `os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Train/Dogs\")`: This creates a `Dogs` folder inside the `Train` directory to store images of dogs used for training.\n",
        "\n",
        "4. **Creating Category-Specific Folders in Testing Data**:\n",
        "   - `os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Test/Cats\")`: This creates a `Cats` folder inside the `Test` directory for storing testing images of cats.\n",
        "   - `os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Test/Dogs\")`: This creates a `Dogs` folder inside the `Test` directory for storing testing images of dogs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LenbZRrz0aOq"
      },
      "outputs": [],
      "source": [
        "#making directories for testing & training data\n",
        "os.mkdir(\"/content/kaggle/Cats-vs-Dogs\")    #Cats-vs-Dogs folder\n",
        "os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Train/\")   #Train folder\n",
        "os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Test/\")    #Test Folder\n",
        "os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Train/Cats\") # Cats Folder in Training data\n",
        "os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Train/Dogs\")  # Dogs Folder in Training data\n",
        "os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Test/Cats\")    # Cats Folder in Testing data\n",
        "os.mkdir(\"/content/kaggle/Cats-vs-Dogs/Test/Dogs\")    # Dogs Folder in Testing data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4pcTWn4dqAh"
      },
      "outputs": [],
      "source": [
        "#keeping track ofrequired dir\n",
        "TRAIN='/content/kaggle/Cats-vs-Dogs/Train/'\n",
        "TEST='/content/kaggle/Cats-vs-Dogs/Test/'\n",
        "\n",
        "CAT_SOURCE='/content/PetImages/Cat/'\n",
        "CAT_TRAIN='/content/kaggle/Cats-vs-Dogs/Train/Cats/'\n",
        "CAT_TEST='/content/kaggle/Cats-vs-Dogs/Test/Cats/'\n",
        "\n",
        "DOG_SOURCE='/content/PetImages/Dog/'\n",
        "DOG_TRAIN='/content/kaggle/Cats-vs-Dogs/Train/Dogs/'\n",
        "DOG_TEST='/content/kaggle/Cats-vs-Dogs/Test/Dogs/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWg4SuqO6FiS"
      },
      "source": [
        " **Splitting and Organizing Dataset**\n",
        "\n",
        "This function, `data_spliting`, is designed to split a dataset into training and testing sets and then organize these subsets into their respective directories. The function also ensures that images are correctly copied into the appropriate folders.\n",
        "\n",
        " **Function Definition**\n",
        "\n",
        "```python\n",
        "def data_spliting(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "  ```\n",
        "  \n",
        "-   **`SOURCE`**: Path to the directory containing the original dataset.\n",
        "-   **`TRAINING`**: Path to the directory where training images will be copied.\n",
        "-   **`TESTING`**: Path to the directory where testing images will be copied.\n",
        "-   **`SPLIT_SIZE`**: Proportion of the data to be allocated to the training set (e.g., 0.8 for 80%).\n",
        "\n",
        " **Detailed Steps**\n",
        "\n",
        "1.  **Filtering Non-Empty Files**:\n",
        "    \n",
        "    -   `file = [filename for filename in os.listdir(SOURCE) if os.path.getsize(SOURCE + filename) > 0]`:\n",
        "        -   This line creates a list of filenames from the `SOURCE` directory, excluding any files that are empty (i.e., size is greater than 0).\n",
        "2.  **Calculating Sizes for Splitting**:\n",
        "    \n",
        "    -   `train_size = int(len(file) * SPLIT_SIZE)`:\n",
        "        -   Determines the number of files to include in the training set based on the `SPLIT_SIZE` proportion.\n",
        "    -   `test_size = int(len(file) * (1 - SPLIT_SIZE))`:\n",
        "        -   Calculates the number of files for the testing set by subtracting the training proportion from 1.\n",
        "3.  **Shuffling and Splitting Data**:\n",
        "    \n",
        "    -   `shuffled_data = random.sample(file, len(file))`:\n",
        "        -   Shuffles the list of files randomly to ensure that the split is not biased by the order of files.\n",
        "    -   `train_data = shuffled_data[0:train_size]`:\n",
        "        -   Extracts the portion of the shuffled list that will be used for training.\n",
        "    -   `test_data = shuffled_data[train_size:]`:\n",
        "        -   Extracts the remaining portion of the shuffled list for testing.\n",
        "4.  **Copying Files to Training Directory**:\n",
        "    \n",
        "    -   `for files in train_data:`:\n",
        "        -   Iterates over the files designated for training.\n",
        "        -   `copyfile(SOURCE + files, f\"{TRAINING}cat-{files}\")`:\n",
        "            -   Copies each training file from the `SOURCE` directory to the `TRAINING` directory, prefixing the filenames with 'cat-' to identify them.\n",
        "5.  **Copying Files to Testing Directory**:\n",
        "    \n",
        "    -   `for files in test_data:`:\n",
        "        -   Iterates over the files designated for testing.\n",
        "        -   `copyfile(SOURCE + files, f\"{TESTING}dog-{files}\")`:\n",
        "            -   Copies each testing file from the `SOURCE` directory to the `TESTING` directory, prefixing the filenames with 'dog-' to identify them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO59zJCYe5TC"
      },
      "outputs": [],
      "source": [
        "def data_spliting(SOURCE,TRAINING,TESTING,SPLIT_SIZE):\n",
        "  file=[filename for filename in os.listdir(SOURCE) if os.path.getsize(SOURCE+filename)>0]\n",
        "  train_size=int(len(file)*SPLIT_SIZE)\n",
        "  test_size=  int(len(file)*(1-SPLIT_SIZE))\n",
        "  shuffled_data=random.sample(file,len(file))\n",
        "  train_data=shuffled_data[0:train_size]\n",
        "  test_data=shuffled_data[train_size:]\n",
        "\n",
        "#copying files into dir Train\n",
        "  for files in train_data:\n",
        "    copyfile(SOURCE+files, f\"{TRAINING}cat-{files}\")\n",
        "\n",
        "#copying files into dir Test\n",
        "\n",
        "  for files in test_data:\n",
        "      copyfile(SOURCE+files, f\"{TESTING}dog-{files}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCK4lJHNi6pT"
      },
      "outputs": [],
      "source": [
        "#spliting data of Cats & Dogs by data_spliting(SOURCE,TRAINING,TESTING,SPLIT_SIZE) function\n",
        "data_spliting(CAT_SOURCE,CAT_TRAIN,CAT_TEST,0.8)\n",
        "data_spliting(DOG_SOURCE,DOG_TRAIN,DOG_TEST,0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAfQw1CVwNlv"
      },
      "outputs": [],
      "source": [
        "#getting length /size of training data\n",
        "print(f\"There are {len(os.listdir(CAT_TRAIN))} images of cats in the training set\")\n",
        "print(f\"There are {len(os.listdir(DOG_TRAIN))} images of dogs in the training set\\n{'_'*50}\")\n",
        "\n",
        "#getting length /size of testing data\n",
        "print(f\"There are {len(os.listdir(CAT_TEST))} images of cats in the testing set\")\n",
        "print(f\"There are {len(os.listdir(DOG_TEST))} images of dogs in the testing set\\n{'_'*50}\")\n",
        "print(f\"The space occupied by 'Dogs' dir in testing data is {os.path.getsize(DOG_TEST)} bytes\")\n",
        "print(f\"The space occupied by 'Cats' dir in testing data is {os.path.getsize(CAT_TEST)} bytes\\n{'_'*50}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbkwg0g57VKx"
      },
      "source": [
        "### **1.4. Your First Task: Creating the Data Generators**\n",
        "\n",
        "Now for your first `TODO`. We need to prepare our data to be fed into the model. We will use `ImageDataGenerator` for this. It does two key things:\n",
        "1.  **Preprocessing:** It will automatically convert our image pixel values from the `[0, 255]` range to the `[0, 1]` range, which is better for training.\n",
        "2.  **Data Augmentation:** It will create modified versions of our training images on-the-fly (by shearing, zooming, and flipping them). This makes our model more robust and helps prevent overfitting.\n",
        "\n",
        "Your task is to use `flow_from_directory` to create a generator for both the training and testing sets.\n",
        "\n",
        "In this section, we use Keras's `ImageDataGenerator` to preprocess and augment images for training and testing our Convolutional Neural Network (CNN). This helps improve model generalization by creating variations of the training images.\n",
        "\n",
        "### Code Explanation\n",
        "\n",
        "```python\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale = 1./255,\n",
        "                shear_range = 0.2,\n",
        "                zoom_range = 0.2,\n",
        "                horizontal_flip = True)\n",
        "```\n",
        "-   **`rescale = 1./255`**:\n",
        "    \n",
        "    -   Normalizes pixel values to the range [0, 1] by scaling them down from the original range [0, 255]. This is a common practice to ensure numerical stability during training.\n",
        "-   **`shear_range = 0.2`**:\n",
        "    \n",
        "    -   Applies random shearing transformations to the images, which can help the model generalize better by making it invariant to slight geometric distortions.\n",
        "-   **`zoom_range = 0.2`**:\n",
        "    \n",
        "    -   Randomly zooms into images, which helps the model to be more robust to variations in object size.\n",
        "-   **`horizontal_flip = True`**:\n",
        "    \n",
        "    -   Randomly flips images horizontally, which helps in training the model to recognize objects regardless of their orientation.\n",
        "\n",
        "### Creating Data Generators\n",
        "\n",
        "**TODO : EXPLAIN THE CODE YOU WRITE HERE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ghxWz5AllHk"
      },
      "outputs": [],
      "source": [
        "#Using ImageDataGenerator from keras\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale = 1./255,\n",
        "                shear_range = 0.2,\n",
        "                zoom_range = 0.2,\n",
        "                horizontal_flip = True)\n",
        "\n",
        "# TODO: Create the training data generator.\n",
        "# HINT: Use the datagen.flow_from_directory() method.\n",
        "# You need to specify the directory (TRAIN), the image size to resize to (target_size),\n",
        "# a good batch size (e.g., 32 or 64), and the class_mode for binary classification.\n",
        "train_set = datagen.flow_from_directory(\n",
        "    directory=____,           # <-- Path to the training data\n",
        "    target_size=(____, ____), # <-- The size your model's input layer expects (256, 256)\n",
        "    batch_size=____,          # <-- How many images in each batch?\n",
        "    class_mode=____         # <-- What kind of classification is this? 'binary'\n",
        ")\n",
        "\n",
        "\n",
        "# TODO: Create the testing data generator.\n",
        "# It should be almost identical to the training generator, but pointing to the TEST directory.\n",
        "test_set = datagen.flow_from_directory(\n",
        "    directory=____,\n",
        "    target_size=(____, ____),\n",
        "    batch_size=____,\n",
        "    class_mode=____\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTPg1zTDGCXT"
      },
      "outputs": [],
      "source": [
        "#Type of test_set\n",
        "type(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NvMLErZ6TKa"
      },
      "source": [
        "To visually inspect the images used for training, we can display random samples from the training directories for both cats and dogs. This helps in verifying that the images are loaded correctly and are as expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMiIee_cjHNr"
      },
      "outputs": [],
      "source": [
        "#displaying the training data of cat\n",
        "img=os.listdir(CAT_TRAIN)\n",
        "\n",
        "keras.preprocessing.image.load_img(CAT_TRAIN+img[random.randint(0,len(img)-1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VcJWBO18q8F"
      },
      "outputs": [],
      "source": [
        "#displaying the training data of dog\n",
        "img= os.listdir(DOG_TRAIN)\n",
        "\n",
        "keras.preprocessing.image.load_img(DOG_TRAIN+img[random.randint(0,len(img)-1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNeTH88hKKkk"
      },
      "source": [
        "# **2. Your Main Task: Building the CNN Architecture**\n",
        "This is the core of the assignment. You will now build the CNN model layer by layer using `keras.models.Sequential`.\n",
        "\n",
        "A standard **Convolutional Block** consists of:\n",
        "1.  A `Conv2D` layer to detect features.\n",
        "2.  A `MaxPooling2D` layer to downsample and reduce dimensionality.\n",
        "\n",
        "We will stack two of these blocks, then `Flatten` the output and add `Dense` layers for the final classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrS9o_5IjrKn"
      },
      "outputs": [],
      "source": [
        "# TODO: Build the CNN Model Architecture\n",
        "# We will build a simple CNN with two convolutional blocks.\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    # --- First Convolutional Block ---\n",
        "    # HINT: A Conv block usually consists of Conv2D -> Activation -> Pooling\n",
        "    # Let's start with 32 filters of size 3x3. Don't forget the input_shape for the first layer!\n",
        "    keras.layers.Conv2D(filters=____, kernel_size=(____,____), activation='relu', input_shape=(256, 256, 3)),\n",
        "    keras.layers.MaxPooling2D(pool_size=(____,____), strides=2),\n",
        "\n",
        "    # --- Second Convolutional Block ---\n",
        "    # Let's increase the number of filters to 64.\n",
        "    keras.layers.Conv2D(filters=____, kernel_size=(____,____), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(____,____), strides=2),\n",
        "\n",
        "    # --- Flatten and Dense Layers ---\n",
        "    # HINT: After the conv blocks, you must Flatten the feature maps before feeding them to Dense layers.\n",
        "    keras.layers.____(),\n",
        "\n",
        "    # A Dense layer for classification. Let's use 128 neurons.\n",
        "    keras.layers.Dense(units=____, activation='relu'),\n",
        "\n",
        "    # The final Output Layer.\n",
        "    # HINT: For binary classification, you need 1 neuron and a 'sigmoid' activation function.\n",
        "    keras.layers.Dense(units=____, activation=____)\n",
        "])\n",
        "\n",
        "# Let's check our work! This will print the model summary.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7zsLpzZPJvM"
      },
      "source": [
        "### **2.1. Task 3: Compiling the Model**\n",
        "Before we can train our model, we need to compile it. This step configures the model for training by specifying its learning process.\n",
        "You have to fill the TODOs below as well in the following code cells\n",
        "\n",
        "* **Optimizer**: 'TODO' is chosen for 'TODO'.\n",
        "\n",
        "* **Loss Function**: 'TODO' is used as the loss function, which is suitable for 'TODO'.\n",
        "\n",
        "* **Metrics:** 'TODO' indicates that the model's performance will be evaluated based on the 'TODO'.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6goJ8Tqok3u7"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=____,\n",
        "    loss=____,\n",
        "    metrics=[____]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JnFQCrwQRIw"
      },
      "source": [
        "\n",
        "### **2.2 Helper Tools: Using Callbacks**\n",
        "To make our training more efficient, we will use two **callbacks**. These are tools that monitor the training process and can take action automatically.\n",
        "- **`EarlyStopping`**: Stops training if the model's performance on the validation set stops improving. This saves time and prevents overfitting.\n",
        "- **`ModelCheckpoint`**: Saves the best version of your model during training.\n",
        "\n",
        "We've provided the code for this below. You don't need to change anything here.\n",
        "And here's the concise explanation of the code\n",
        " 1. **EarlyStopping**\n",
        "\n",
        "-   **Purpose:** Stops the training process when a monitored metric (like validation loss) stops improving.\n",
        "-   **How it works:**\n",
        "    -   It monitors a specific metric (e.g., validation loss).\n",
        "    -   If the metric doesn't improve for a specified number of epochs (patience), training is stopped to prevent overfitting.\n",
        "    -   This helps in avoiding unnecessary training, saving time and computational resources.\n",
        "\n",
        " 2. **ModelCheckpoint**\n",
        "\n",
        "-   **Purpose:** Saves the model (or its weights) at specific intervals during training.\n",
        "-   **How it works:**\n",
        "    -   It monitors a metric (like validation accuracy) and saves the model whenever the metric improves.\n",
        "    -   You can specify conditions like saving only the best model, saving after every epoch, or saving based on other criteria.\n",
        "    -   This ensures that you always have the best-performing model saved, even if the training deteriorates later.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgEiwRKDHK3P"
      },
      "outputs": [],
      "source": [
        "#CallBacks\n",
        "#EarlyStopping\n",
        "early_stop= ES(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0.001,\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='auto',\n",
        "    baseline=None)\n",
        "\n",
        "#ModelCheckpoint\n",
        "checkpoint= MCP(\n",
        "    filepath='best_model.weights.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True ,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    save_freq=\"epoch\",\n",
        "    initial_value_threshold=None,\n",
        "               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOZft1SYAtXy"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abfj7o0wgKip"
      },
      "source": [
        "### **2.3. Task 4: Training the Model!**\n",
        "It's time for the magic to happen. We will now \"fit\" our model to our training data. This is the step where the model learns the patterns that distinguish cats from dogs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ulWP1MdmAxn"
      },
      "outputs": [],
      "source": [
        "# TODO: Train (fit) the model.\n",
        "# HINT: Use the model.fit() method.\n",
        "# You need to provide:\n",
        "# 1. The training data (our train_set generator).\n",
        "# 2. The validation data (our test_set generator) to monitor performance on unseen data.\n",
        "# 3. The number of 'epochs' (how many times to go through the entire dataset). Let's start with 15.\n",
        "# 4. The 'callbacks' we defined earlier to use EarlyStopping and ModelCheckpoint.\n",
        "\n",
        "track = model.fit(\n",
        "    ____,                    # <-- Your training data generator\n",
        "    validation_data=____,    # <-- Your testing data generator\n",
        "    epochs=____,             # <-- How many epochs to train for?\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x-01UaIKMQQ"
      },
      "outputs": [],
      "source": [
        "print(track.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCjNjHBd8FXO"
      },
      "source": [
        "# **3. Task # 5: Evaluating Your Model's Performance**\n",
        "Training is complete! But how well did our model actually do? The final step is to evaluate its performance on the test set—data it has never seen before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkeXRUuSgKis"
      },
      "source": [
        "#### **3.1. Making Predictions**\n",
        "First, we need to use our trained model to make predictions on the test set. The model will output probabilities, which we then need to convert into our final `0` (Cat) or `1` (Dog) labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3leupqKZ8Vik"
      },
      "outputs": [],
      "source": [
        "# TODO: Predict labels for the entire test set.\n",
        "# HINT: Use the model.predict() method on our 'test_set'.\n",
        "predictions_probabilities = model.predict(____)\n",
        "\n",
        "# The model outputs probabilities (numbers between 0 and 1).\n",
        "# We need to convert them to binary labels (0 or 1).\n",
        "# If the probability is > 0.5, we'll classify it as 1 (Dog), otherwise 0 (Cat).\n",
        "# HINT: You can do this with a simple comparison like (predictions_probabilities > 0.5)\n",
        "predicted_labels = (____ > 0.5).astype(int).flatten()\n",
        "\n",
        "# Get the true labels for comparison\n",
        "true_labels = test_set.classes\n",
        "\n",
        "# The cells below will now use your 'true_labels' and 'predicted_labels'\n",
        "# to show you how well your model performed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T13EsmaKji6W"
      },
      "source": [
        "### **3.2. Model Evaluation Using Confusion Matrix and Classification Report**\n",
        "In this section we'll evaluate our CNN using Confusion Matrix and Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ2loFp8xz28"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(test_set_labels, test_set_preds)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Cat', 'Dog'], yticklabels=['Cat', 'Dog'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbXvvu21kity"
      },
      "source": [
        "Classification Report\n",
        "The code snippet uses the classification_report function from the sklearn.metrics module to evaluate the performance of a classification model.\n",
        "\n",
        "**Key Components of the Output:**\n",
        "\n",
        "**Precision:** The model has high precision (0.98-1.0) and recall (0.98-1.0) for all classes, indicating low false positives and false negatives.\n",
        "\n",
        "**Recall:** The ratio of true positive predictions to the total actual positives. It measures the model's ability to identify all relevant instances.\n",
        "\n",
        "**F1-Score:** The F1-score (harmonic mean of precision and recall) ranges from 0.98 to 0.99, suggesting a good balance between precision and recall.\n",
        "\n",
        "**Support:** The support column shows the number of instances for each class in the test set (e.g., 980 instances for class 0, 1135 for class 1, etc.).\n",
        "\n",
        ">The overall accuracy is 0.99, meaning the model correctly classifies 99% of the instances in the test set.\n",
        "\n",
        ">The macro avg and weighted avg rows provide the average metrics across all classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3_jV_JP32wB"
      },
      "outputs": [],
      "source": [
        "report = classification_report(test_set_labels, test_set_preds, target_names=['Cat', 'Dog'])\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h8dhtYQmeZy"
      },
      "source": [
        " **Graph b/w Model Accuracy & Loss**\n",
        "Following graph shows that the model accuracy increases and loss decreases with progressing epochs from 1 to 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9WyCKWV1Ws0"
      },
      "outputs": [],
      "source": [
        "plt.plot(track.history['loss'])\n",
        "plt.plot(track.history['val_loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss', 'val_loss'], loc='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq5EL9l0EGZV"
      },
      "source": [
        "# **4. Task # 6: Conclusion and Reflection**\n",
        "In this final section, analyze the results from your model evaluation. Answer the following questions in a new text cell below.\n",
        "\n",
        "1.  **Final Performance:** What was the final accuracy and loss of your model on the test set?\n",
        "2.  **Confusion Matrix:** What do the numbers in your confusion matrix tell you? Did your model have more trouble identifying cats or dogs?\n",
        "3.  **Training Curves:** Look at the \"Model Accuracy & Loss\" graphs. Does it look like your model was starting to overfit, or could it have trained for more epochs?\n",
        "4.  **Potential Improvements:** If you had more time, what is one thing you would try to change to potentially improve your model's performance? (e.g., add more layers, change the learning rate, use more data augmentation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aVbfVO_2h77"
      },
      "source": [
        "## ***Final Task : Q/A's***\n",
        "---\n",
        "## Answers of the following conceptual Questions\n",
        "\n",
        "1. What is the advantages of Convolutional Layers over Fully Connected Layers?\n",
        "2. What is the role of Pooling Layers in reducing complexity of a CNN ?\n",
        "3. Draw the Comparison of Pooling Layers?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3eKPYJzgKjL"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Replace this with the URL of the GIF you want to display\n",
        "gif_url = \"https://cardsimages.info-tuparada.com/566/4555-6-congratulations-ecard.gif\"\n",
        "\n",
        "# Display the GIF\n",
        "display(Image(url=gif_url))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}